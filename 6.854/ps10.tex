\documentclass[psamsfonts]{amsart}

%-------Packages---------
\usepackage{amssymb,amsfonts}
\usepackage{enumerate}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm}
\usepackage{theorem}
\usepackage{verbatim}
\usepackage{framed}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

\newenvironment{sol}{\vspace{0.25cm}{\large \bfseries Solution:}}{\qedsymbol}
\newenvironment{prob}[1]{\begin{framed}{\large \bfseries Problem #1:}}{\end{framed}}
\newcommand{\makenewtitle}{
    \begin{center}
    {\huge \bfseries 6.854 Advanced Algorithms} \\
    Problem Set 10\\
    \vspace{0.25cm}
    {\bfseries John Wang} \\
    Collaborators:  
    \end{center}
    \vspace{0.5cm}
}


\bibliographystyle{plain}

\voffset = -10pt
\headheight = 0pt
\topmargin = -20pt
\textheight = 690pt

\begin{document}

\makenewtitle

\begin{prob}{1-a}
Prove that LRU and FIFO are conservative.
\end{prob}
\begin{sol}
First, we will consider LRU. Suppose that the LRU cache of size $k$ has been filled up and is entering a new phase. Now consider a subsequence that contains at most $k$ pages. Consider pages $p_1, p_2, \ldots, p_k$ and LRU will check each whether $p_i$ is already in the cache. If it is, then there is a cache hit. Otherwise, there is a fault. However, since there are at most $k$ pages in the input sequence, there can be at most $k$ faults. Therefore, we see that LRU is conservative.

Now, let us examine FIFO with the same analysis. Consider a subsequence of pages $p_1, p_2, \ldots, p_k$. We know that there are exactly $k$ pages in the input sequence. Since each page in the sequence can cause at most a single fault, there can be a maximum of $k$ faults on FIFO. THerefore, we see that FIFO is conservative as well.
\end{sol}

\begin{prob}{1-b}
Prove that any conservative algorithm is $k$-competitive
\end{prob}
\begin{sol}
Suppose that some algorithm $A$ is conservative. We will break up the conservative algorithm's operation into phases of size $k$. Now suppose that the algorithm has just started phase $i$. In this phase, there will be $k$ page requests. Since the algorithm is conservative, $A$, will have a maximum of $k$ faults in this upcoming phase (which is really a subsequence of $k$ pages). Now let us suppose OPT does not fault in phase $i$. Then it must fault on the $k+1$st request (first request in the next phase) since the cache is only of size $k$ and it must have all $k$ requests already stored in the cache. If OPT does fault during phase $i$, then it has at least one fault. Therefore, for each phase $i$, there will be at least one fault by OPT and at most $k$ faults by $A$. 

Since this is true for each phase $i$, we see that $A$ is $k$-competitive, and hence, that any conservative algorithm is $k$-competitive.
\end{sol}

\newpage
\makenewtitle

\begin{prob}{2-a}
Show that DC-Tree is $k$-competitive.
\end{prob}
\begin{sol}
Suppose that OPT moves by some distance $d$. We will use the potential function $\Phi = kM + \sum_{i < j} d(s_i, s_j)$ as defined in class. Now, when OPT moves, the optimum matching increases by at most $d$, since there is now a possibility off adding a cost of $d$ to the matching. The distance term does not change, so the change in potential is given by $\Delta \Phi = kd$. 

Now suppose that DC-Tree moves by the same distance $d$. We will break up the move into phases, where during each phase, the number of neighbors is fixed. Suppose that in phase $i$, there are $n_i$ neighbors who each travel $l_i$ distance. We can break down the DC-Tree's change in potential into two cases.

First, suppose that $n_i = 1$ so that there is only a single server moving a distance of $l_i$ towards the request. Note that this is the final part of the phase, and the $l_i$ will cover the final distance so that the last server moving ends up at the request. Since OPT's server has already moved to the request point, the minimum matching between DC-Tree and OPT decreases by $l_i$ during this phase. This is because the last moving server can be matched with OPT's point at the request (by the uncrossing argument given in class). Moreover, the distance term $\sum_{i < j} d(s_i, s_j)$increases by at most $(k-1) l_i$ because there are only $k-1$ servers that are further away from the last server. This means that the change in potential is given by $\Delta \Phi \leq -k l_i + (k-1) l_i = - l_i$. 

Now suppose that $n_i > 1$. From the uncrossing argument given in class, there exists a minimum matching between the servers in DC-Tree and OPT such that OPT's server on the request is matched with the closest server moving towards it in DC-Tree. Thus, one of the servers is decreasing the cost of the matching by $l_i$ while the other moving servers could be increasing their matchings by $l_i$. The change in the matching is then $-l_i + (n_i - 1) l_i$. The moving servers will all be decreasing their distance to each other (since they are all moving towards the request). Since there are ${n_i \choose 2}$ pairs of servers that are moving closer, each by a distance of $2l_i$, the distance decreases by ${n_i \choose 2} 2 l_i$. However, there are also $k - n_i$ stationary servers which change their distance to servers. For each stationary server, the distance to $n_i - 1$ of the servers decreases by $l_i$, and increases by $l_i$ for a single server. This means that the total distance decreases by ${n_i \choose 2} 2l_i + ((n_i-1)l_i - l_i) l_i (k - n_i)$. The change in potential for phase $i$ is then:
\begin{eqnarray}
\Delta \Phi_i &=& k (n_i-2)l_i - n_i(n_i-1)l_i - (n_i-2)l_i(k-n_i) \\
&=& l_i (n_i - 2) n_i - n_i( n_i-1) l_i \\
&=& -l_i n_i
\end{eqnarray}

Since during each phase, we know that $n_i \geq 1$, we must have that $\Delta \Phi =  \sum_{i} \Delta \Phi_i \leq \sum_{i} -d = -d$. This means that during each move by DC-Tree, the potential decreases by at least $d$, whereas each one of OPT's moves increases the potential by at most $kd$. This shows that DC-Tree is $k$-competitive.
\end{sol}

\begin{prob}{2-b}
Show that any algorithm for $k$-server on a tree can be used to solve the paging problem by modeling paging as $k$-server on a particularly simple tree. 
\end{prob}
\begin{sol}
There will be a single leaf for each page that is to be requested. The tree will look like a standard binary search tree, with $p$ leaves (one for each possible page). There will be $k$ servers which correspond to cache locations. If a server resides on a leaf, then that leaf will be considered to be in the cache. If a server does not reside on a leaf, then the previous leaf that it resided on (if there exists one) will be the page that is in the cache. 

Thus, as soon as a server $s_j$ reaches a new leaf corresponding to page $p_i$, then the page $p_i$ will be brought into the cache and will remove whatever used to be in cache location $j$. 
\end{sol}

\begin{prob}{2-c}
What standard paging algorithm do you get when you apply the above reduction using DC-Tree.
\end{prob}
\begin{sol}
Notice that all neighbors of the request will move towards the leaf with the reuest. Eventually, however, there can only be a single server moving to the request and actually reaching the request location. Moreover, we see that the final server to reach the request is the closest server to the request (in terms of number of edges travelled). If we assume that the pages are placed as leaves in a random ordering, then we can think of this mechanism as randomly choosing a node to eject from the cache.

Thus, we can think of the DC-Tree algorithm as the randomized marking algorithm for paging in the following manner. Whenever a server $s_j$ is sitting on a leaf corresponding to page $p_i$, then that page is marked. If the server is no longer sitting on the page, then the page becomes unmarked, but could possibly still be in the cache. 

Thus, if all pages are marked so that all $k$ servers are at some leaf in the tree, and a new request comes, we must have a fault in the cache and we also will see each of the $k$ servers starting to move towards the request. This means that all of the previously marked servers will be unmarked. This corresponds exactly to the randomized marking algorithm which unmarks all the pages if there is a fault and there are already $k$ pages marked. 
\end{sol}


\newpage
\makenewtitle

\begin{prob}{3-a}
Show that any deterministic strategy for choosing dates and deciding whether to break up is terrible from a competitive perspective: you can be forced by fate to end up with the absolutely worst possible choice.
\end{prob}
\begin{sol}

\end{sol}

\end{document}
